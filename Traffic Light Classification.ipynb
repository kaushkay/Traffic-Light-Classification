{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Traffic Light Classification.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "R3s4HCpgnAMC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math, random\n",
        "import os, sys, time\n",
        "import itertools\n",
        "import cv2\n",
        "import glob\n",
        "# The glob module finds all the pathnames matching a specified pattern according \n",
        "# to the rules used by the Unix shell, although results are returned in arbitrary order. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JHZmLe3xnAM9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Basic Parameters\n",
        "\n",
        "max_epochs = 25\n",
        "base_image_path =  \"images/\"\n",
        "image_types = [\"red\", \"green\", \"yellow\"]\n",
        "input_image_x = 32\n",
        "input_image_y = 32\n",
        "train_test_split_ratio = 0.9\n",
        "batch_size = 32\n",
        "checkpoint_name = \"model.ckpt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9JtxWIcFnANB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Helper layer Funtion:\n",
        "\n",
        "def weight_variable(shape):\n",
        "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def bias_variable(shape):\n",
        "    initial = tf.constant(0.1, shape=shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def conv2d(x, W, stride):\n",
        "    return tf.nn.conv2d(x, W, strides=[1,stride,stride,1], padding='SAME')\n",
        "\n",
        "def max_pool_2x2(x):\n",
        "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EVe8_PFBnANE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1. Our input neurons will be the shape of the image which is (32 x 32 x 3)\n",
        "2. Because our data will be one-hot encoded, we have as many output neurons as we have classes"
      ]
    },
    {
      "metadata": {
        "id": "kpfYFraInANF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Model\n",
        "\n",
        "x = tf.placeholder(tf.float32, shape=[None, input_image_x, input_image_y, 3])\n",
        "y_ = tf.placeholder(tf.float32, shape=[None, len(image_types)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LuTCgRyMnANI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is where we specify our first convolutional layers."
      ]
    },
    {
      "metadata": {
        "id": "bv2AbaVRnANJ",
        "colab_type": "code",
        "outputId": "c0dba3aa-97ea-4694-e190-203f4e9cd25e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "x_image = x\n",
        "\n",
        "# Our first 3 conv layers, of 16 3X3 filters\n",
        "W_conv1 = weight_variable([3, 3, 3, 16])\n",
        "b_conv1 = bias_variable([16])\n",
        "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1, 1) + b_conv1)\n",
        "\n",
        "W_conv2 = weight_variable([3, 3, 16, 16])\n",
        "b_conv2 = bias_variable([16])\n",
        "h_conv2 = tf.nn.relu(conv2d(h_conv1, W_conv2, 1) + b_conv2)\n",
        "\n",
        "W_conv3 = weight_variable([3, 3, 16, 16])\n",
        "b_conv3 = bias_variable([16])\n",
        "h_conv3 = tf.nn.relu(conv2d(h_conv2, W_conv3, 1) + b_conv3)\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "lzM6fj4Wp6Dz",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Our Pooling layer\n",
        "h_pool4 = max_pool_2x2(h_conv3)\n",
        "\n",
        "n1, n2, n3, n4 = h_pool4.get_shape().as_list()\n",
        "\n",
        "W_fc1 = weight_variable([n2*n3*n4, 3])\n",
        "b_fc1 = bias_variable([3])\n",
        "\n",
        "# We flatten our pool layer into a fully connected layer\n",
        "\n",
        "h_pool4_flat = tf.reshape(h_pool4, shape=[-1, n2*n3*n4])\n",
        "\n",
        "y = tf.matmul(h_pool4_flat, W_fc1) + b_fc1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wxES91nRnANO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sess = tf.InteractiveSession()    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qvog8dUInANS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<l>*Our loss function is defined as computing softmax, and then cross entropy.*\n",
        "<l>*We also specify our optimizer, which takes a learning rate, and a loss function.*\n",
        "<l>*Finally, we initialize all of our variables which will tell us if our model is valid.*"
      ]
    },
    {
      "metadata": {
        "id": "ds7RQ9WcnANT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Our loss function and optimizer\n",
        "\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, \n",
        "                                                                 logits=y))\n",
        "train_step = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gByY4Mr8nANV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We need to load in our images. We do so using OpenCV's imread function. After loading in each image, we resize it to our input size."
      ]
    },
    {
      "metadata": {
        "id": "nN7jD2OInANW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "With each loaded image, we also specify the expected output. For this, we use a one-hot encoding, creating an array of zeros represnting each class, and setting the index of the expected class number to 1"
      ]
    },
    {
      "metadata": {
        "id": "FOFnoGGJnANW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For example, if we have three classes, and we expect an order of: [red neuron, green neuron, yellow neuron] \n",
        "We initialize an array to [0, 0, 0] and if we loaded a yellow light, we change the last value to 1: [0, 0, 1] \n",
        "Finally, we shuffle our dataset. (It's generally useful to seed our random generator with 0 at the start of the program)"
      ]
    },
    {
      "metadata": {
        "id": "dNi7iBN7nANX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "saver = tf.train.Saver()\n",
        "time_start = time.time()\n",
        "\n",
        "v_loss = least_loss = 99999999\n",
        "\n",
        "# Load data\n",
        "\n",
        "full_set = []\n",
        "\n",
        "for im_type in image_types:\n",
        "    for ex in glob.glob(os.path.join(base_image_path, im_type, \"*\")):\n",
        "        im = cv2.imread(ex)\n",
        "        if not im is None:\n",
        "            im = cv2.resize(im, (32, 32))\n",
        "\n",
        "            # Create an array representing our classes and set it\n",
        "            one_hot_array = [0] * len(image_types)\n",
        "            one_hot_array[image_types.index(im_type)] = 1\n",
        "            assert(im.shape == (32, 32, 3))\n",
        "\n",
        "            full_set.append((im, one_hot_array, ex))\n",
        "\n",
        "random.shuffle(full_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vQ8DGY2Ux9Me",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Using our train_test_split_ratio we create two lists of examples: testing and training."
      ]
    },
    {
      "metadata": {
        "id": "b07MSRRLx94v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "13dd4368-4917-4452-9c8b-6fec5d8664a8"
      },
      "cell_type": "code",
      "source": [
        "# We split our data into a training and test set here\n",
        "split_index = int(math.floor(len(full_set) * train_test_split_ratio))\n",
        "train_set = full_set[: split_index]\n",
        "test_set = full_set[split_index :]\n",
        "\n",
        "\n",
        "# We ensure that our training and test sets are a multiple of batch size\n",
        "train_set_offset = len(train_set) % batch_size\n",
        "test_set_offset = len(test_set) % batch_size\n",
        "\n",
        "train_set = train_set[:len(train_set)-train_set_offset]\n",
        "test_set = test_set[: len(test_set) - test_set_offset]\n",
        "\n",
        "x_train, y_train, z_train = zip(*train_set)\n",
        "x_test, y_test, z_test = zip(*test_set)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-528d3e66e890>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtest_set_offset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 0)"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "rl9UQyWRNlRp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VFPJFYb8NSxT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M9Rhf7pKNCFo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uGV9yEF-NOR8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tgxirpgvNFJh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}